[
  {
    "objectID": "posts/Random_Forest/Random_Forest.html",
    "href": "posts/Random_Forest/Random_Forest.html",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "",
    "text": "In this post, I will use Random Forest method to predict default rate."
  },
  {
    "objectID": "posts/Random_Forest/Random_Forest.html#data",
    "href": "posts/Random_Forest/Random_Forest.html#data",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "Data",
    "text": "Data\nTo answer the question, I will analyze loan data for 5000 banking system customers in a Middle Eastern country.\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/rtava/MLProject/master/posts/Random_Forest/data_loans.csv\")\n\n\nprint(\"This dataset has\", df.shape[0], \"records with\", df.shape[1], \"atributes\")\nprint(\"About\", df.label.mean().round(2) , \"of lables was defaulted with label 1\") \n\nThis dataset has 5578 records with 16 atributes\nAbout 0.38 of lables was defaulted with label 1"
  },
  {
    "objectID": "posts/Random_Forest/Random_Forest.html#varibales",
    "href": "posts/Random_Forest/Random_Forest.html#varibales",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "Varibales",
    "text": "Varibales\nThe following table provide a description for variables in the data set.\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nage\nAge of borrower\n\n\ninstall\nNumber of requested loan’s installment\n\n\namount\nNumber of requested loan\n\n\ndelayed\ntotal delyaed that the borrower had\n\n\nneg2y\nNumber of negative status in recent two years\n\n\nneg2yb\nNumber of negative status before recent two years\n\n\npd_6m\nNumber of past due payment in recent 6 months\n\n\npd_618\nNumber of past due payment in recent 6-18 months\n\n\npd_18m\nNumber of past due payment before recent 18 months\n\n\nud_12m\nNumber of unpaid payment in recent 12 months\n\n\nud_12mm\nNumber of unpaid payment before recent 12 months\n\n\nnloans\nNumber of terminated loan the person have had\n\n\nloanAge\nThe lodest loan age\n\n\nremain\nOutstanding debt to total debt (ratio)\n\n\nwneg_2y\nWorst negative status in recent two years\n\n\nlabel\nis =1 if the costumer paid at least one installment with more than 90 days delay"
  },
  {
    "objectID": "posts/Random_Forest/Random_Forest.html#descriptive-statistics",
    "href": "posts/Random_Forest/Random_Forest.html#descriptive-statistics",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"whitegrid\")\n\ndf.loc[df['install']&gt;61, 'Installment_Group']=\"less than 60\"\ndf.Installment_Group.fillna(\"more than 60\", inplace=True)\n\n\nplot = sns.scatterplot(x=\"install\", y=\"amount\",\n                s=5, palette=\"ch:r=-.2,d=.3_r\",\n                sizes=(1, 8), linewidth=0, hue=\"Installment_Group\",\n                data=df);\nplot.set_xlabel(\"Number of installment\")\nplot.set_ylabel(\"Loan Amount\")\nplot.set_title(\"Relation between loan amount and number of installment\")\n\nplt.show();\n\nC:\\Users\\rtava\\AppData\\Local\\Temp\\ipykernel_11384\\1438381614.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'less than 60' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[df['install']&gt;61, 'Installment_Group']=\"less than 60\"\n\n\n\n\n\n\nDealing with outliers\nThe following lines remove outliers using interquartile range method:\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf.drop(columns='Installment_Group', inplace=True)\ndf_new = df\n\nfor c in df_new.columns:\n    q1 = df_new[c].quantile(0.05)\n    q3 = df_new[c].quantile(0.95)\n    iqr = q3-q1\n    lower_fence = q1 - 1.5*iqr\n    upper_fence = q3 + 1.5*iqr\n\n    for i in range(len(df)):\n        if df_new.loc[i, c] &lt; lower_fence or df_new.loc[i, c] &gt; upper_fence:  # if outlier\n            df_new.loc[i, \"out_\" + c] = 1\n        else:\n            df_new.loc[i, \"out_\" + c] = 0\n\nout_col = [col for col in df_new.columns if 'out_' in col]\ndf_new['out'] = df_new[out_col].sum(axis=1)\n\ndf_new = df_new.loc[df_new.out==0]\ndf_new.drop([col for col in df_new.columns if 'out_' in col], axis=1, inplace=True)\ndf_new.drop(columns=['out'], inplace=True)\n\nI change the categorical dummies to usual dummies:\n\ndf_prep = pd.get_dummies(df_new, columns=['wneg_2y'])\n\nfor column in ['wneg_2y_0.0', \n          'wneg_2y_1.0', \n          'wneg_2y_2.0',\n          'wneg_2y_3.0']:\n        df_prep[column] = df_prep[column].replace({True: 1, False: 0})\n\n\n\nCorrelation between variabels:\n\ncorr = df_prep.corr(method = 'pearson')\ncorr\nsns.heatmap(corr,annot=False,fmt=\".1f\", linewidth=.5)\n\n&lt;Axes: &gt;\n\n\n\n\n\nBased on the heat map correlation, I will remove “wneg” variables.\n\nx_col = ['age', 'install', 'amount', 'delayed', 'neg2y', 'neg2yb', 'pd_6m',\n       'pd_618', 'pd_18m', 'ud_12m', 'ud_12mm', 'nloans', 'loanAge',\n       'remain']"
  },
  {
    "objectID": "posts/Random_Forest/Random_Forest.html#model-evaluation",
    "href": "posts/Random_Forest/Random_Forest.html#model-evaluation",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nAccuracy_RF = metrics.accuracy_score(y_test, y_pred_RF)\nF1_RF = metrics.f1_score(y_test, y_pred_RF)\nAUC = metrics.roc_auc_score(y_test, y_pred_RF)\nclas_report = metrics.classification_report(y_test, y_pred_RF)\n\nprint(\"RF Accuracy: %0.2f\" % (Accuracy_RF), \"\\nF1 Score: %0.2f\" % (F1_RF), \"\\nAUC_ROC: %0.2f\" % (AUC))\n\nRF Accuracy: 0.69 \nF1 Score: 0.49 \nAUC_ROC: 0.63\n\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncf_matrix = confusion_matrix(y_test, y_pred_RF)\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='0.1f')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nOptimal Threshold\n\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\n\nFscore = []\nthrsh = []\nfor i in range(10,65,1):\n    threshold = i/100\n    thrsh.append(threshold)\n\n    RF_model = RandomForestClassifier(random_state=1234, max_depth = 20, n_estimators=100, class_weight='balanced')\n    RF_model.fit(X_train,y_train)\n    y_prob_RF = RF_model.predict_proba(X_test)\n    y_pred_RF = (y_prob_RF[:,1] &gt;= threshold).astype('int')\n    Accuracy_RF = metrics.accuracy_score(y_test, y_pred_RF)\n    Fscore.append(metrics.f1_score(y_test, y_pred_RF))\n\n\nplot = sns.lineplot(x=thrsh, y=Fscore)\nplt.scatter(x=0.3, y=0.586, color='r')\nplot.set_xlabel(\"Threshold\")\nplot.set_ylabel(\"F1 Score\");\n\n\n\n\nWe can see that the optimal threshold is 0.3, then we have:\n\nthreshold = 0.3\ny_pred_RF = (y_prob_RF[:,1] &gt;= threshold).astype('int')\n\nAccuracy_RF = metrics.accuracy_score(y_test, y_pred_RF)\nF1_RF = metrics.f1_score(y_test, y_pred_RF)\nAUC = metrics.roc_auc_score(y_test, y_pred_RF)\nclas_report = metrics.classification_report(y_test, y_pred_RF)\n\nprint(\"RF Accuracy: %0.2f\" % (Accuracy_RF), \"\\nF1 Score: %0.2f\" % (F1_RF), \"\\nAUC_ROC: %0.2f\" % (AUC))\ncf_matrix = confusion_matrix(y_test, y_pred_RF)\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='0.1f');\n\nRF Accuracy: 0.61 \nF1 Score: 0.59 \nAUC_ROC: 0.64"
  },
  {
    "objectID": "posts/Logistic_Regrssion/Logistic Regression.html",
    "href": "posts/Logistic_Regrssion/Logistic Regression.html",
    "title": "Customer Loyalty, Using Logistic Regression",
    "section": "",
    "text": "In this blog post, I will use a telecommunications dataset for predicting customer churn."
  },
  {
    "objectID": "posts/Logistic_Regrssion/Logistic Regression.html#descriptive-statistics",
    "href": "posts/Logistic_Regrssion/Logistic Regression.html#descriptive-statistics",
    "title": "Customer Loyalty, Using Logistic Regression",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"whitegrid\")\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\nax1, ax2, ax3, ax4 = axes.flatten()\n\nsns.barplot(data=df, x=\"ed\", y=\"churn\", \n            label=\"Churned Costumers\", \n            color=\"b\", ax=ax1)\n\nsns.histplot(data=df, x='tenure', hue='churn',\n             color=\"b\", palette=\"light:b_r\",\n             multiple=\"stack\" ,edgecolor=\".3\", ax=ax2)\n\nsns.histplot(data=df, x='age', hue='churn',\n             color=\"b\", palette=\"light:b_r\",\n             multiple=\"stack\",edgecolor=\".3\", ax=ax3)\n\nsns.barplot(data=df,x=\"churn\", y=\"income\", \n            label=\"Costumers Income\", color=\"b\", ax=ax4)\n\nplt.tight_layout()\nplt.show();\n\n\n\n\n\nCorrelation between variabeles\n\ncorr = df.corr(method = 'pearson')\nsns.heatmap(corr,fmt=\".1f\", linewidth=.2, cmap=\"crest\", annot=True, annot_kws={\"size\": 9});\n\n\n\n\nBased on the heat map correlation, I will keep all variables bacause all correlation are less than 80%. But I will scale my data by removing the mean and scaling to unit variance.\n\nx_col = ['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',\n       'callcard', 'wireless']\n\n\nfrom sklearn import preprocessing\ndf[x_col] = preprocessing.StandardScaler().fit(df[x_col]).transform(df[x_col])"
  },
  {
    "objectID": "posts/Logistic_Regrssion/Logistic Regression.html#model-evaluation",
    "href": "posts/Logistic_Regrssion/Logistic Regression.html#model-evaluation",
    "title": "Customer Loyalty, Using Logistic Regression",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nfrom sklearn import metrics\n\nAccuracy_RF = metrics.accuracy_score(y_test, y_pred_LR)\nF1_RF = metrics.f1_score(y_test, y_pred_LR)\nAUC = metrics.roc_auc_score(y_test, y_pred_LR)\nclas_report = metrics.classification_report(y_test, y_pred_LR)\n\nprint(\"RF Accuracy: %0.2f\" % (Accuracy_RF), \"\\nF1 Score: %0.2f\" % (F1_RF), \"\\nAUC_ROC: %0.2f\" % (AUC))\n\nRF Accuracy: 0.84 \nF1 Score: 0.71 \nAUC_ROC: 0.80\n\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncf_matrix = confusion_matrix(y_test, y_pred_LR)\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='0.1f')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nfrom sklearn.metrics import log_loss\nlog_loss(y_test, y_prob_LR)\n\n0.5073750448656873"
  },
  {
    "objectID": "posts/DBSCAN/DBSCAN.html",
    "href": "posts/DBSCAN/DBSCAN.html",
    "title": "Outlier Detection methods in ML",
    "section": "",
    "text": "In the field of Machine Learning, there exist various techniques to identify outliers. In this blog post, I will be demonstrating the use of IQR and DBSCAN methods to detect outliers.\nFor the purpose of this demonstration, I will be utilizing a dataset that contains information on the height and weight of 200 individuals.\n\nimport pandas as pd\ndf = pd.read_csv(\"https://gist.githubusercontent.com/simonlast/d20a53af004bfd7cd912403884ce80d2/raw/c92846e3387f11abc7e318db88c7aca6ecec1b4c/height_weight_data.csv\")\ndf.rename(columns = {' Height (Inches)':'Height', ' Weight (Pounds)': 'Weight'}, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nIndex\nHeight\nWeight\n\n\n\n\n0\n1\n65.78\n112.99\n\n\n1\n2\n71.52\n136.49\n\n\n2\n3\n69.40\n153.03\n\n\n3\n4\n68.22\n142.34\n\n\n4\n5\n67.79\n144.30\n\n\n\n\n\n\n\n\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\nsns.scatterplot(data = df, x = \"Height\", y = \"Weight\")\n\n&lt;Axes: xlabel='Height', ylabel='Weight'&gt;"
  },
  {
    "objectID": "posts/DBSCAN/DBSCAN.html#defining-quatiles",
    "href": "posts/DBSCAN/DBSCAN.html#defining-quatiles",
    "title": "Outlier Detection methods in ML",
    "section": "1. Defining quatiles",
    "text": "1. Defining quatiles\nFirst, we should derive quatiles of a variable. I use 75% and 25% quartiles.\n\n\n\nBoxplot\n\n\n\nq1_h = df['Height'].quantile(0.25)\nq3_h = df['Height'].quantile(0.75)\n\nq1_w = df['Weight'].quantile(0.25)\nq3_w = df['Weight'].quantile(0.75)"
  },
  {
    "objectID": "posts/DBSCAN/DBSCAN.html#iqr-range",
    "href": "posts/DBSCAN/DBSCAN.html#iqr-range",
    "title": "Outlier Detection methods in ML",
    "section": "2. IQR Range",
    "text": "2. IQR Range\nI define a new variable to detect datapoint between 25% and 75% quantiles, then, create a range to keep the datapoints in this range.\n\\(Fence_{Lower} = Q1 - \\alpha \\times (Q3-Q1)\\)\n\\(Fence_{Upper} = Q3 + \\alpha \\times (Q3-Q1)\\)\n\nalpha = 0.5\n\niqr_h = q3_h - q1_h\nlower_fence_h = q1_h - alpha*iqr_h\nupper_fence_h = q3_h + alpha*iqr_h\n\niqr_w = q3_w - q1_w\nlower_fence_w = q1_w - alpha*iqr_w\nupper_fence_w = q3_w + alpha*iqr_w\n\n\n\n\nIQR"
  },
  {
    "objectID": "posts/DBSCAN/DBSCAN.html#detecting-outliers",
    "href": "posts/DBSCAN/DBSCAN.html#detecting-outliers",
    "title": "Outlier Detection methods in ML",
    "section": "3. Detecting Outliers",
    "text": "3. Detecting Outliers\nIn this step, I label the datapoint that are out of the range.\n\nfor i in range(len(df)):\n    if df.loc[i, 'Height'] &lt; lower_fence_h or df.loc[i, 'Height'] &gt; upper_fence_h:  # if outlier\n        df.loc[i, \"out_\" + 'Height'] = 1\n    else:\n            df.loc[i, \"out_\" + 'Height'] = 0\n\n\nfor i in range(len(df)):\n    if df.loc[i, 'Weight'] &lt; lower_fence_w or df.loc[i, 'Weight'] &gt; upper_fence_w:  # if outlier\n        df.loc[i, \"out_\" + 'Weight'] = 1\n    else:\n            df.loc[i, \"out_\" + 'Weight'] = 0\n\ndf['outliers'] = 0\ndf.loc[(df['out_Height'] == 1) & (df['out_Weight'] == 1), 'outliers'] = 1\n\n\ndf.outliers.mean()\n\n0.045\n\n\n\nsns.scatterplot(data = df, \n                x = \"Height\", \n                y = \"Weight\", \n                hue='outliers').set_title(\"IQR Outlier detection\");"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m Reza Tavakoli - a PhD student in Economics at Virginia Tech University. I have a keen interest in using various tools to answer serious questions. Through this blog, I will be sharing material related to introducing different kinds of econometric and machine learning methods, using these methods to answer important questions that matter."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLProject",
    "section": "",
    "text": "Outlier Detection methods in ML\n\n\n\n\n\n\n\nML\n\n\nDBSCAN\n\n\nIQR\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nReza Tavakoli\n\n\n\n\n\n\n  \n\n\n\n\nA simple price categorization system for used cell phones, using KMean method\n\n\n\n\n\n\n\nML\n\n\nKMean\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2023\n\n\nReza Tavakoli\n\n\n\n\n\n\n  \n\n\n\n\nChance Regularities in Casting Two Dices\n\n\n\n\n\n\n\nML\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nReza Tavakoli\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Loyalty, Using Logistic Regression\n\n\n\n\n\n\n\nML\n\n\nRegression\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nReza Tavakoli\n\n\n\n\n\n\n  \n\n\n\n\nUsing Random Forest to predict loans defualt rate\n\n\n\n\n\n\n\nML\n\n\nRandom Forest\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nReza Tavakoli\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/K_Mean/K-Mean.html",
    "href": "posts/K_Mean/K-Mean.html",
    "title": "A simple price categorization system for used cell phones, using KMean method",
    "section": "",
    "text": "In this blog post, I will discuss a straightforward system for categorizing the prices of used cell phones in an online store. The goal is to use the K-Mean method to allocate cell phones to different price categories based on their mobile specifications."
  },
  {
    "objectID": "posts/K_Mean/K-Mean.html#optimal-number-of-clusters",
    "href": "posts/K_Mean/K-Mean.html#optimal-number-of-clusters",
    "title": "A simple price categorization system for used cell phones, using KMean method",
    "section": "Optimal Number of Clusters",
    "text": "Optimal Number of Clusters\nIn this step, I calculate the “K-mean Inertia” for different numbers of clusters from 1 to 20, and plot the values. Based on the results, I can determine the optimal number of clusters to categorize cell phones.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nkmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(x_data)\n                for k in range(1, 21)]\ninertias = [model.inertia_ for model in kmeans_per_k]\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\na = sns.lineplot(x= range(1, 21), y = inertias)\nb = sns.scatterplot(x= range(1, 21), y = inertias)\n\nb.set_xticks(range(1,21)) \nb.set_xlabel(\"Number of Clusters\")\nb.set_ylabel(\"Cost Function (Inertia)\")\n\nplt.show();\n\n\n\n\nI set number of cluster to 6\n\nk_means = KMeans(init = \"k-means++\", n_clusters = 6, \n                 n_init=12, random_state=42)\n\nk_means.fit(x_data)\ndf['Cluster'] = k_means.labels_\n\nThe following plot shows the relation between clusterring, internal memory, and ram.\n\nimport seaborn as sns\ng = sns.scatterplot(data=df, x=\"int_memory\", y=\"ram\", hue=\"Cluster\")\ng.set_xlabel(\"Internal Memory\")\ng.set_ylabel(\"Random Access Memory\")\n\nplt.show();"
  },
  {
    "objectID": "posts/Probability Theory/Probability Theory.html",
    "href": "posts/Probability Theory/Probability Theory.html",
    "title": "Chance Regularities in Casting Two Dices",
    "section": "",
    "text": "In this blog post, I will show how chance regularities plays roll in Probability Theory.\n\nimport random\nimport numpy as np\n\n\nfirst_dice = []\nelement_50 = []\n\nfor i in range(0,50):\n    element_50.append(i)\n    first_dice.insert(i, random.randint(1,6))\n\nsecond_dice = []\nfor i in range(0,50):\n    second_dice.insert(i, random.randint(1,6))\n\nsum_dice_50 = [first_dice[i] + second_dice[i] for i in range(len(second_dice))]\n\n\nimport seaborn as sns\nplot = sns.lineplot(x =element_50, y=sum_dice_50, marker=\"o\",lw=1)\nplot.set_xlabel(\"Order of casting two dices\")\nplot.set_ylabel(\"Sum\")\n\nText(0, 0.5, 'Sum')\n\n\n\n\n\n\nsns.histplot(x=sum_dice_50, kde=True, discrete=True)\n\n&lt;Axes: ylabel='Count'&gt;\n\n\n\n\n\n\n\nfor j in [100, 1000, 10000]:\n    first_dice = []\n    globals()['element_%s' % j] = []\n    \n    for i in range(0,j):\n        globals()['element_%s' % j].append(i)\n        first_dice.insert(i, random.randint(1,6))\n\n    second_dice = []\n    for i in range(0,j):\n        second_dice.insert(i, random.randint(1,6))\n\n    globals()['sum_dice_%s' % j] = [first_dice[i] + second_dice[i] for i in range(len(second_dice))]\n\n\n\nprob_50 = {}\nfor i in range(1,13):\n    prob_50[i] = round(sum_dice_50.count(i)/50, ndigits=2)\n\nprob_10000 = {}\nfor i in range(1,13):\n    prob_10000[i] = round(sum_dice_10000.count(i)/10000,ndigits=2)\n\nprint(prob_50)\nprint(prob_10000)\n\n{1: 0.0, 2: 0.04, 3: 0.04, 4: 0.08, 5: 0.04, 6: 0.14, 7: 0.18, 8: 0.18, 9: 0.08, 10: 0.1, 11: 0.1, 12: 0.02}\n{1: 0.0, 2: 0.03, 3: 0.06, 4: 0.08, 5: 0.11, 6: 0.14, 7: 0.16, 8: 0.14, 9: 0.11, 10: 0.09, 11: 0.06, 12: 0.03}\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"whitegrid\")\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(9, 12))\nax1, ax2, ax3, ax4, ax5, ax6, ax7,ax8 = axes.flatten()\n\n\nsns.lineplot(x =element_50, y=sum_dice_50, lw=0.5 ,ax=ax1)\nsns.histplot(x=sum_dice_50, kde=True, discrete=True,ax=ax2).set_title(\"50 sample\")\nsns.lineplot(x =element_100, y=sum_dice_100, lw=0.5,ax=ax3)\nsns.histplot(x=sum_dice_100, kde=True, discrete=True,ax=ax4).set_title(\"100 sample\")\nsns.lineplot(x =element_1000, y=sum_dice_1000, lw=0.5 ,ax=ax5)\nsns.histplot(x=sum_dice_1000, kde=True, discrete=True,ax=ax6).set_title(\"1,000 sample\")\nsns.lineplot(x =element_10000, y=sum_dice_10000,lw=0.1,ax=ax7)\nsns.histplot(x=sum_dice_10000, kde=True, discrete=True,ax=ax8, kde_kws={\"bw_adjust\":2}).set_title(\"10,000 sample\")\n\n\nplt.tight_layout()\nplt.show();"
  }
]