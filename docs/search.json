[
  {
    "objectID": "posts/Random_Forest/Random_Forest.html",
    "href": "posts/Random_Forest/Random_Forest.html",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "",
    "text": "In this post, I will use Random Forest method to predict default rate."
  },
  {
    "objectID": "posts/Random_Forest/Random_Forest.html#data",
    "href": "posts/Random_Forest/Random_Forest.html#data",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "Data",
    "text": "Data\nTo answer the question, I will analyze loan data for 5000 banking system customers in a Middle Eastern country.\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/rtava/MLProject/master/posts/Random_Forest/data_loans.csv\")\n\n\nprint(\"This dataset has\", df.shape[0], \"records with\", df.shape[1], \"atributes\")\nprint(\"About\", df.label.mean().round(2) , \"of lables was defaulted with label 1\") \n\nThis dataset has 5578 records with 16 atributes\nAbout 0.38 of lables was defaulted with label 1"
  },
  {
    "objectID": "posts/Random_Forest/Random_Forest.html#varibales",
    "href": "posts/Random_Forest/Random_Forest.html#varibales",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "Varibales",
    "text": "Varibales\nThe following table provide a description for variables in the data set.\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nage\nAge of borrower\n\n\ninstall\nNumber of requested loan’s installment\n\n\namount\nNumber of requested loan\n\n\ndelayed\ntotal delyaed that the borrower had\n\n\nneg2y\nNumber of negative status in recent two years\n\n\nneg2yb\nNumber of negative status before recent two years\n\n\npd_6m\nNumber of past due payment in recent 6 months\n\n\npd_618\nNumber of past due payment in recent 6-18 months\n\n\npd_18m\nNumber of past due payment before recent 18 months\n\n\nud_12m\nNumber of unpaid payment in recent 12 months\n\n\nud_12mm\nNumber of unpaid payment before recent 12 months\n\n\nnloans\nNumber of terminated loan the person have had\n\n\nloanAge\nThe lodest loan age\n\n\nremain\nOutstanding debt to total debt (ratio)\n\n\nwneg_2y\nWorst negative status in recent two years\n\n\nlabel\nis =1 if the costumer paid at least one installment with more than 90 days delay"
  },
  {
    "objectID": "posts/Random_Forest/Random_Forest.html#descriptive-statistics",
    "href": "posts/Random_Forest/Random_Forest.html#descriptive-statistics",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"whitegrid\")\n\ndf.loc[df['install']&gt;61, 'Installment_Group']=\"less than 60\"\ndf.Installment_Group.fillna(\"more than 60\", inplace=True)\n\n\nplot = sns.scatterplot(x=\"install\", y=\"amount\",\n                s=5, palette=\"ch:r=-.2,d=.3_r\",\n                sizes=(1, 8), linewidth=0, hue=\"Installment_Group\",\n                data=df);\nplot.set_xlabel(\"Number of installment\")\nplot.set_ylabel(\"Loan Amount\")\nplot.set_title(\"Relation between loan amount and number of installment\")\n\nplt.show();\n\nC:\\Users\\rtava\\AppData\\Local\\Temp\\ipykernel_11384\\1438381614.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'less than 60' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[df['install']&gt;61, 'Installment_Group']=\"less than 60\"\n\n\n\n\n\n\nDealing with outliers\nThe following lines remove outliers using interquartile range method:\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf.drop(columns='Installment_Group', inplace=True)\ndf_new = df\n\nfor c in df_new.columns:\n    q1 = df_new[c].quantile(0.05)\n    q3 = df_new[c].quantile(0.95)\n    iqr = q3-q1\n    lower_fence = q1 - 1.5*iqr\n    upper_fence = q3 + 1.5*iqr\n\n    for i in range(len(df)):\n        if df_new.loc[i, c] &lt; lower_fence or df_new.loc[i, c] &gt; upper_fence:  # if outlier\n            df_new.loc[i, \"out_\" + c] = 1\n        else:\n            df_new.loc[i, \"out_\" + c] = 0\n\nout_col = [col for col in df_new.columns if 'out_' in col]\ndf_new['out'] = df_new[out_col].sum(axis=1)\n\ndf_new = df_new.loc[df_new.out==0]\ndf_new.drop([col for col in df_new.columns if 'out_' in col], axis=1, inplace=True)\ndf_new.drop(columns=['out'], inplace=True)\n\nI change the categorical dummies to usual dummies:\n\ndf_prep = pd.get_dummies(df_new, columns=['wneg_2y'])\n\nfor column in ['wneg_2y_0.0', \n          'wneg_2y_1.0', \n          'wneg_2y_2.0',\n          'wneg_2y_3.0']:\n        df_prep[column] = df_prep[column].replace({True: 1, False: 0})\n\n\n\nCorrelation between variabels:\n\ncorr = df_prep.corr(method = 'pearson')\ncorr\nsns.heatmap(corr,annot=False,fmt=\".1f\", linewidth=.5)\n\n&lt;Axes: &gt;\n\n\n\n\n\nBased on the heat map correlation, I will remove “wneg” variables.\n\nx_col = ['age', 'install', 'amount', 'delayed', 'neg2y', 'neg2yb', 'pd_6m',\n       'pd_618', 'pd_18m', 'ud_12m', 'ud_12mm', 'nloans', 'loanAge',\n       'remain']"
  },
  {
    "objectID": "posts/Random_Forest/Random_Forest.html#model-evaluation",
    "href": "posts/Random_Forest/Random_Forest.html#model-evaluation",
    "title": "Using Random Forest to predict loans defualt rate",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nAccuracy_RF = metrics.accuracy_score(y_test, y_pred_RF)\nF1_RF = metrics.f1_score(y_test, y_pred_RF)\nAUC = metrics.roc_auc_score(y_test, y_pred_RF)\nclas_report = metrics.classification_report(y_test, y_pred_RF)\n\nprint(\"RF Accuracy: %0.2f\" % (Accuracy_RF), \"\\nF1 Score: %0.2f\" % (F1_RF), \"\\nAUC_ROC: %0.2f\" % (AUC))\n\nRF Accuracy: 0.69 \nF1 Score: 0.49 \nAUC_ROC: 0.63\n\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncf_matrix = confusion_matrix(y_test, y_pred_RF)\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='0.1f')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nOptimal Threshold\n\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\n\nFscore = []\nthrsh = []\nfor i in range(10,65,1):\n    threshold = i/100\n    thrsh.append(threshold)\n\n    RF_model = RandomForestClassifier(random_state=1234, max_depth = 20, n_estimators=100, class_weight='balanced')\n    RF_model.fit(X_train,y_train)\n    y_prob_RF = RF_model.predict_proba(X_test)\n    y_pred_RF = (y_prob_RF[:,1] &gt;= threshold).astype('int')\n    Accuracy_RF = metrics.accuracy_score(y_test, y_pred_RF)\n    Fscore.append(metrics.f1_score(y_test, y_pred_RF))\n\n\nplot = sns.lineplot(x=thrsh, y=Fscore)\nplt.scatter(x=0.3, y=0.586, color='r')\nplot.set_xlabel(\"Threshold\")\nplot.set_ylabel(\"F1 Score\");\n\n\n\n\nWe can see that the optimal threshold is 0.3, then we have:\n\nthreshold = 0.3\ny_pred_RF = (y_prob_RF[:,1] &gt;= threshold).astype('int')\n\nAccuracy_RF = metrics.accuracy_score(y_test, y_pred_RF)\nF1_RF = metrics.f1_score(y_test, y_pred_RF)\nAUC = metrics.roc_auc_score(y_test, y_pred_RF)\nclas_report = metrics.classification_report(y_test, y_pred_RF)\n\nprint(\"RF Accuracy: %0.2f\" % (Accuracy_RF), \"\\nF1 Score: %0.2f\" % (F1_RF), \"\\nAUC_ROC: %0.2f\" % (AUC))\ncf_matrix = confusion_matrix(y_test, y_pred_RF)\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='0.1f');\n\nRF Accuracy: 0.61 \nF1 Score: 0.59 \nAUC_ROC: 0.64"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/KNN/KNN.html",
    "href": "posts/KNN/KNN.html",
    "title": "Chance Regularities in Casting Two Dices",
    "section": "",
    "text": "In this blog post, I will show how chance regularities plays roll in Probability theory.\nthis is `a = 20*20` a code\n\nimport random\nimport numpy as np\n\n\nfirst_dice = []\nelement_50 = []\n\nfor i in range(0,50):\n    element_50.append(i)\n    first_dice.insert(i, random.randint(1,6))\n\nsecond_dice = []\nfor i in range(0,50):\n    second_dice.insert(i, random.randint(1,6))\n\nsum_dice_50 = [first_dice[i] + second_dice[i] for i in range(len(second_dice))]\n\n\nimport seaborn as sns\nplot = sns.lineplot(x =element_50, y=sum_dice_50, marker=\"o\",lw=1)\nplot.set_xlabel(\"Order of casting two dices\")\nplot.set_ylabel(\"Sum\")\n\nText(0, 0.5, 'Sum')\n\n\n\n\n\n\nsns.histplot(x=sum_dice_50, kde=True, discrete=True)\n\n&lt;Axes: ylabel='Count'&gt;\n\n\n\n\n\n\n\nfor j in [100, 1000, 10000]:\n    first_dice = []\n    globals()['element_%s' % j] = []\n    \n    for i in range(0,j):\n        globals()['element_%s' % j].append(i)\n        first_dice.insert(i, random.randint(1,6))\n\n    second_dice = []\n    for i in range(0,j):\n        second_dice.insert(i, random.randint(1,6))\n\n    globals()['sum_dice_%s' % j] = [first_dice[i] + second_dice[i] for i in range(len(second_dice))]\n\n\n\nprob_50 = {}\nfor i in range(1,13):\n    prob_50[i] = round(sum_dice_50.count(i)/50, ndigits=2)\n\nprob_10000 = {}\nfor i in range(1,13):\n    prob_10000[i] = round(sum_dice_10000.count(i)/10000,ndigits=2)\n\nprint(prob_50)\nprint(prob_10000)\n\n{1: 0.0, 2: 0.04, 3: 0.04, 4: 0.08, 5: 0.04, 6: 0.14, 7: 0.18, 8: 0.18, 9: 0.08, 10: 0.1, 11: 0.1, 12: 0.02}\n{1: 0.0, 2: 0.03, 3: 0.06, 4: 0.08, 5: 0.11, 6: 0.14, 7: 0.16, 8: 0.14, 9: 0.11, 10: 0.09, 11: 0.06, 12: 0.03}\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"whitegrid\")\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(9, 12))\nax1, ax2, ax3, ax4, ax5, ax6, ax7,ax8 = axes.flatten()\n\n\nsns.lineplot(x =element_50, y=sum_dice_50, lw=0.5 ,ax=ax1)\nsns.histplot(x=sum_dice_50, kde=True, discrete=True,ax=ax2).set_title(\"50 sample\")\nsns.lineplot(x =element_100, y=sum_dice_100, lw=0.5,ax=ax3)\nsns.histplot(x=sum_dice_100, kde=True, discrete=True,ax=ax4).set_title(\"100 sample\")\nsns.lineplot(x =element_1000, y=sum_dice_1000, lw=0.5 ,ax=ax5)\nsns.histplot(x=sum_dice_1000, kde=True, discrete=True,ax=ax6).set_title(\"1,000 sample\")\nsns.lineplot(x =element_10000, y=sum_dice_10000,lw=0.1,ax=ax7)\nsns.histplot(x=sum_dice_10000, kde=True, discrete=True,ax=ax8, kde_kws={\"bw_adjust\":2}).set_title(\"10,000 sample\")\n\n\nplt.tight_layout()\nplt.show();"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m Reza Tavakoli - a PhD student in Economics at Virginia Tech University. I have a keen interest in using various tools to answer serious questions. Through this blog, I will be sharing material related to introducing different kinds of econometric and machine learning methods, using these methods to answer important questions that matter."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLProject",
    "section": "",
    "text": "Chance Regularities in Casting Two Dices\n\n\n\n\n\n\n\nML\n\n\nKNN\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nReza Tavakoli\n\n\n\n\n\n\n  \n\n\n\n\nChance Regularities in Casting Two Dices\n\n\n\n\n\n\n\nML\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nReza Tavakoli\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Loyalty, Using Logistic Regression\n\n\n\n\n\n\n\nML\n\n\nRegression\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nReza Tavakoli\n\n\n\n\n\n\n  \n\n\n\n\nUsing Random Forest to predict loans defualt rate\n\n\n\n\n\n\n\nML\n\n\nRandom Forest\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nReza Tavakoli\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Logistic_Regrssion/Logistic Regression.html",
    "href": "posts/Logistic_Regrssion/Logistic Regression.html",
    "title": "Customer Loyalty, Using Logistic Regression",
    "section": "",
    "text": "In this blog post, I will use a telecommunications dataset for predicting customer churn."
  },
  {
    "objectID": "posts/Logistic_Regrssion/Logistic Regression.html#descriptive-statistics",
    "href": "posts/Logistic_Regrssion/Logistic Regression.html#descriptive-statistics",
    "title": "Customer Loyalty, Using Logistic Regression",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"whitegrid\")\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\nax1, ax2, ax3, ax4 = axes.flatten()\n\nsns.barplot(data=df, x=\"ed\", y=\"churn\", \n            label=\"Churned Costumers\", \n            color=\"b\", ax=ax1)\n\nsns.histplot(data=df, x='tenure', hue='churn',\n             color=\"b\", palette=\"light:b_r\",\n             multiple=\"stack\" ,edgecolor=\".3\", ax=ax2)\n\nsns.histplot(data=df, x='age', hue='churn',\n             color=\"b\", palette=\"light:b_r\",\n             multiple=\"stack\",edgecolor=\".3\", ax=ax3)\n\nsns.barplot(data=df,x=\"churn\", y=\"income\", \n            label=\"Costumers Income\", color=\"b\", ax=ax4)\n\nplt.tight_layout()\nplt.show();\n\n\n\n\n\nCorrelation between variabeles\n\ncorr = df.corr(method = 'pearson')\nsns.heatmap(corr,fmt=\".1f\", linewidth=.2, cmap=\"crest\", annot=True, annot_kws={\"size\": 9});\n\n\n\n\nBased on the heat map correlation, I will keep all variables bacause all correlation are less than 80%. But I will scale my data by removing the mean and scaling to unit variance.\n\nx_col = ['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',\n       'callcard', 'wireless']\n\n\nfrom sklearn import preprocessing\ndf[x_col] = preprocessing.StandardScaler().fit(df[x_col]).transform(df[x_col])"
  },
  {
    "objectID": "posts/Logistic_Regrssion/Logistic Regression.html#model-evaluation",
    "href": "posts/Logistic_Regrssion/Logistic Regression.html#model-evaluation",
    "title": "Customer Loyalty, Using Logistic Regression",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nfrom sklearn import metrics\n\nAccuracy_RF = metrics.accuracy_score(y_test, y_pred_LR)\nF1_RF = metrics.f1_score(y_test, y_pred_LR)\nAUC = metrics.roc_auc_score(y_test, y_pred_LR)\nclas_report = metrics.classification_report(y_test, y_pred_LR)\n\nprint(\"RF Accuracy: %0.2f\" % (Accuracy_RF), \"\\nF1 Score: %0.2f\" % (F1_RF), \"\\nAUC_ROC: %0.2f\" % (AUC))\n\nRF Accuracy: 0.84 \nF1 Score: 0.71 \nAUC_ROC: 0.80\n\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncf_matrix = confusion_matrix(y_test, y_pred_LR)\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='0.1f')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nfrom sklearn.metrics import log_loss\nlog_loss(y_test, y_prob_LR)\n\n0.5073750448656873"
  },
  {
    "objectID": "posts/Probability Theory/Probability Theory.html",
    "href": "posts/Probability Theory/Probability Theory.html",
    "title": "Chance Regularities in Casting Two Dices",
    "section": "",
    "text": "In this blog post, I will show how chance regularities plays roll in Probability theory.\nthis is `a = 20*20` a code\n\nimport random\nimport numpy as np\n\n\nfirst_dice = []\nelement_50 = []\n\nfor i in range(0,50):\n    element_50.append(i)\n    first_dice.insert(i, random.randint(1,6))\n\nsecond_dice = []\nfor i in range(0,50):\n    second_dice.insert(i, random.randint(1,6))\n\nsum_dice_50 = [first_dice[i] + second_dice[i] for i in range(len(second_dice))]\n\n\nimport seaborn as sns\nplot = sns.lineplot(x =element_50, y=sum_dice_50, marker=\"o\",lw=1)\nplot.set_xlabel(\"Order of casting two dices\")\nplot.set_ylabel(\"Sum\")\n\nText(0, 0.5, 'Sum')\n\n\n\n\n\n\nsns.histplot(x=sum_dice_50, kde=True, discrete=True)\n\n&lt;Axes: ylabel='Count'&gt;\n\n\n\n\n\n\n\nfor j in [100, 1000, 10000]:\n    first_dice = []\n    globals()['element_%s' % j] = []\n    \n    for i in range(0,j):\n        globals()['element_%s' % j].append(i)\n        first_dice.insert(i, random.randint(1,6))\n\n    second_dice = []\n    for i in range(0,j):\n        second_dice.insert(i, random.randint(1,6))\n\n    globals()['sum_dice_%s' % j] = [first_dice[i] + second_dice[i] for i in range(len(second_dice))]\n\n\n\nprob_50 = {}\nfor i in range(1,13):\n    prob_50[i] = round(sum_dice_50.count(i)/50, ndigits=2)\n\nprob_10000 = {}\nfor i in range(1,13):\n    prob_10000[i] = round(sum_dice_10000.count(i)/10000,ndigits=2)\n\nprint(prob_50)\nprint(prob_10000)\n\n{1: 0.0, 2: 0.04, 3: 0.04, 4: 0.08, 5: 0.04, 6: 0.14, 7: 0.18, 8: 0.18, 9: 0.08, 10: 0.1, 11: 0.1, 12: 0.02}\n{1: 0.0, 2: 0.03, 3: 0.06, 4: 0.08, 5: 0.11, 6: 0.14, 7: 0.16, 8: 0.14, 9: 0.11, 10: 0.09, 11: 0.06, 12: 0.03}\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"whitegrid\")\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(9, 12))\nax1, ax2, ax3, ax4, ax5, ax6, ax7,ax8 = axes.flatten()\n\n\nsns.lineplot(x =element_50, y=sum_dice_50, lw=0.5 ,ax=ax1)\nsns.histplot(x=sum_dice_50, kde=True, discrete=True,ax=ax2).set_title(\"50 sample\")\nsns.lineplot(x =element_100, y=sum_dice_100, lw=0.5,ax=ax3)\nsns.histplot(x=sum_dice_100, kde=True, discrete=True,ax=ax4).set_title(\"100 sample\")\nsns.lineplot(x =element_1000, y=sum_dice_1000, lw=0.5 ,ax=ax5)\nsns.histplot(x=sum_dice_1000, kde=True, discrete=True,ax=ax6).set_title(\"1,000 sample\")\nsns.lineplot(x =element_10000, y=sum_dice_10000,lw=0.1,ax=ax7)\nsns.histplot(x=sum_dice_10000, kde=True, discrete=True,ax=ax8, kde_kws={\"bw_adjust\":2}).set_title(\"10,000 sample\")\n\n\nplt.tight_layout()\nplt.show();"
  }
]